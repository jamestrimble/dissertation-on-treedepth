\algnewcommand{\IfNDebug}[1]{#1}
%\algnewcommand{\IfNDebug}[1]{}

\chapter{Swapping Graphs and Two-Sided Branching in \McSplit}
\label{c:swapping-graphs-mcsplit}

\section{Introduction}

A maximum common subgraph of $G$ and $H$ is clearly
also a maximum common subgraph of $H$ and $G$.  In this chapter we examine whether
it can be useful to swap the two input graphs when calling an MCIS solver.
We will see that swapping the graphs can have a very large effect on the
run time of \McSplit\
if the two graphs differ in density or order.  The second contribution of this
chapter is to introduce \McSplit-2S, a modified
version of \McSplit\ that generalises the idea of swapping graphs by choosing
at each search node which side of a label class to branch on.
Our experimental evaluation finds that this algorithm improves upon \McSplit,
albeit modestly, for some classes of graphs.

\Cref{sec:when-swap} introduces two simple and very effective heuristics for
deciding whether to swap graphs.  \Cref{sec:explaining-sd-so} speculates about
reasons for the success of the swapping rules.  \Cref{sec:mcsplit2} introduces
\McSplit-2S, a version of \McSplit\ that can branch on two sides.
\Cref{sec:swapping-related-work} discusses related work, and
\Cref{sec:swapping-conclusion} concludes.

\section{When Should we Swap the Graphs?}\label{sec:when-swap}

We refer to the swapping
version of \McSplit\ as \McSplit-Swap; this is shown in
\Cref{McSplitSwapAlg}. \McSplit-Swap simply calls \McSplit\ with the two
graphs interchanged, then returns the resulting mapping with its elements reversed.
We begin by plotting run times of \McSplit\ with and without swapping graphs, to
give an idea of the difference that swapping can make.

\begin{algorithm}[h!]
\DontPrintSemicolon
\nl $\FuncSty{McSplit-Swap}(G, H)$ \label{McSplitSwapFun} \;
\nl \Begin{
    \nl $M \gets \FuncSty{McSplit}(H,G)$ \LeftComment{Call \McSplit\ with the graphs swapped}\;
    \nl $\KwSty{return}$~$\{(w, v) \mid (v, w) \in M\}$ \LeftComment{Reverse the mapping}
}
    \caption{\McSplit-Swap: a version of \McSplit\ that swaps the input graphs.} 
\label{McSplitSwapAlg}
\end{algorithm}

\Cref{subfig:runtime-swapping-scatter-mcsplain} shows run times for unlabelled,
undirected instances from the database described in
\Cref{sec:mcsplit-instances}; we will refer to these as the MCS Plain
instances.  As with all experiments in this chapter, the plot use a
random sample of 1000 of the benchmark instances.  The horizontal axis shows
run time in milliseconds without swapping the graphs, and the vertical axis
shows run time with swapping.  Although swapping graphs can make a small
difference, it does not change run time by as much as an order of magnitude for
any of these instances.

\begin{figure}[htb]
    \centering
    \subfigure[][MCS Plain instances] {
        \centering
        \includegraphics*[width=0.42\textwidth]{14-mcsplit-i-undirected/modified-mcsplit-experiment/plots/plots/left-vs-right-mcsplain-jittered}
        \label{subfig:runtime-swapping-scatter-mcsplain}
    }
    \subfigure[][Random varying-density instances] {
        \centering
        \includegraphics*[width=0.42\textwidth]{14-mcsplit-i-undirected/modified-mcsplit-experiment/plots/plots/left-vs-right-random2-jittered}
        \label{subfig:runtime-swapping-scatter-random2}
    }
    \subfigure[][Random varying-order instances] {
        \centering
        \includegraphics*[width=0.42\textwidth]{14-mcsplit-i-undirected/modified-mcsplit-experiment/plots/plots/left-vs-right-random3-jittered}
        \label{subfig:runtime-swapping-scatter-random3}
    }
    \caption{Run times in ms for \McSplit\ (horizontal axis) and \McSplit-Swap
        (vertical axis).  Each point represents an instance.  Swapping the graphs
        has little effect on run time
        for MCS Plain instances, but changes the run time by orders of magnitude
        for many of the random instances.}\label{figure:runtime-swapping-scatter}
\end{figure}

The fact that swapping makes little difference to run time on the MCS Plain
instances can be explained by the way these instances are generated.  Within
each MCS Plain instance, the two graphs are produced using the same graph
generator, have the same number of vertices, and have very similar density
(\Cref{figure:mcsplain-densities}).  Given the similarity of the two graphs in
each of these instances, it is unsurprising that swapping the graphs has little
effect on run time.

To examine pairs of graphs with very \emph{different} characteristics, we
generated two sets of random instances using the Erd\H{o}s-Rényi $G(n,p)$ model.
The first set has 24 vertices per graph, with the $p$ parameter of the
generator chosen randomly from the interval $[0,1]$; thus, the graphs have the
same order but vary greatly in density.  The second set of instances has the
density parameter $p$ set to $0.3$ for all instances, with the number of
vertices in each graph randomly chosen from $\{10, 11, \dots, 50\}$.  These
instances will be used throughout this chapter; we call them
\emph{varying-density instances} and the \emph{varying-order instances}
respectively.

\begin{figure}[htb]
    \centering
    \includegraphics*[width=0.4\textwidth]{14-mcsplit-i-undirected/modified-mcsplit-experiment/plots/plots/mcsplain-densities}
    \caption{In each pair of MCS plain instances, the two graphs have very similar densities. The figure
        plots the densities of graphs $G$ and $H$ in each graph pair.}
    \label{figure:mcsplain-densities}
\end{figure}

\Cref{subfig:runtime-swapping-scatter-random2} shows run times with and without swapping
for the varying-density instances.
For some of these instances swapping graphs is very clearly beneficial, and for others,
swapping greatly increases the run time.
\Cref{subfig:runtime-swapping-scatter-random3} shows run times with and without swapping
for the varying-order instances.  Here again we see that swapping can be beneficial,
albeit to a lesser degree.

We have seen that \McSplit\ and
\McSplit-Swap can have very different run times.  Are there heuristics we can
use to choose the better algorithm for a given instance? We now turn to this
question, beginning with the varying-density instances.

\subsection{Random Graphs With Fixed $n$ and Varying Density}

Can we tell in advance whether we should swap the graphs of a given instance?
\Cref{figure:coloured-scatter-run-times-density} shows that, in the case of our
random varying-density instances, there is a very strong association between the densities
of the two graphs and whether we should swap graphs.  In this plot, the axes
measure graph density,\footnote{By \emph{density}, we are referring to the actual density of a graph,
$\frac{|E(G)|}{n_G(n_G-1)/2}$, rather than the parameter $p$ used to generate
the graph. In practice the two measures are very similar.}. Colour is used to show
the effect of swapping graphs on run time; red dots represent those instances
that are solved more quickly with the graphs swapped.  The diagonal lines on the plot
show the curves $y=x$ and $y=1-x$; these divide the plot into four triangular regions.
The figure shows clearly that instances lying
in the upper and lower triangles tend to be solved more quickly by \McSplit, while
those lying in the left and right triangles tend to be solved more
quickly by \McSplit-Swap.

\begin{figure}[htb]
    \centering
    \subfigure[][Density] {
        \centering
        \includegraphics*[width=0.44\textwidth]{14-mcsplit-i-undirected/modified-mcsplit-experiment/plots/plots/density-when-swap}
        \label{figure:coloured-scatter-run-times-density}
    }
    \subfigure[][``Extremeness'' of density] {
        \centering
        \includegraphics*[width=0.44\textwidth]{14-mcsplit-i-undirected/modified-mcsplit-experiment/plots/plots/density-extremeness-when-swap}
        \label{figure:coloured-scatter-run-times-extremeness}
    }
    \caption{There is a strong association between the densities of graphs $G$ and $H$
        and whether it is beneficial to use \McSplit-Swap rather than \McSplit.
        The first subfigure plots density of $G$ against density of $H$, with one point
        plotted for each of the random varying-density instances.  Instances for which \McSplit-Swap
        is faster are plotted in red; instances where \McSplit\ is faster
        are plotted in blue.  Dark red and dark blue indicate that \McSplit-Swap and \McSplit,
        respectively, result in run times at least twice as fast as the alternative.}
        \label{figure:coloured-scatter-mcis-run-times}
\end{figure}

The union of the left and right triangles in
\Cref{figure:coloured-scatter-run-times-density} has a simple characterisation.
Define the measure \emph{density extremeness} of
a graph as $\left|\frac{1}{2} - d\right|$, where $d$ is the graph's
density.  This measures how close a graph's
density is to either 0 or 1; a clique and an independent set
have the highest possible density extremeness.  The union of the left and right triangles
in \Cref{figure:coloured-scatter-run-times-density} contains exactly those
graph pairs $(G,H)$ such that the density extremeness of $G$ is greater than
the density extremeness of $H$.
The second subfigure, \Cref{figure:coloured-scatter-run-times-extremeness},
replots the data in \Cref{figure:coloured-scatter-run-times-density},
with density extremeness rather than density measured on the axes.
It is evident from the figure that \McSplit-Swap almost always
runs faster than \McSplit\ on instances where the density extremeness of $G$
exceeds the density extremeness of $H$.

Given the strong relationship between the densities of $G$ and $H$ and the 
optimal order in which to pass the graphs to \McSplit, a natural next
step is to devise a version of \McSplit\ that swaps the graphs
if and only if the density extremeness of $G$ is greater
than the density extremeness of $H$.  We call this algorithm
\McSplit-SD (with SD standing for ``swapping on density''). It is shown in \Cref{McSplitSDAlg}.

\begin{algorithm}[h!]
\DontPrintSemicolon
\nl $\FuncSty{McSplit-SD}(G, H)$ \label{McSplitSDFun} \;
\nl \Begin{
    \nl \If{$\left|\frac{1}{2} - d_G\right| > \left|\frac{1}{2} - d_H\right|$}{
        \nl $\KwSty{return}$~$\FuncSty{McSplit-Swap}(G,H)$ \;
    }
    \nl $\KwSty{return}$~$\FuncSty{McSplit}(G,H)$
}
    \caption{\McSplit-SD: a version of \McSplit\ that uses density to decide whether to swap the input graphs.} 
\label{McSplitSDAlg}
\end{algorithm}

\Cref{figure:left-vs-smart-d-mcis} compares on our set of varying-density random instances
the run times of three solvers: \McSplit, \McSplit-SD, and the virtual best solver (VBS) of
\McSplit\ and \McSplit-Swap. The VBS time is computed by running \McSplit\ and
\McSplit-Swap, then recording the lower of the two run times.  It can be viewed as the run time
of an algorithm that uses an oracle to determine whether to swap the two graphs.
The cumulative plot shows that \McSplit-Swap and the VBS
have almost identical performance overall, and that both outperform \McSplit.  The scatter
plot in \Cref{figure:left-vs-smart-random2} shows that \McSplit-SD is much slower than
\McSplit\ on only a single instance, and is orders of magnitude faster on several of the
instances.  In summary, for this family of instances, \McSplit-SD clearly improves upon
\McSplit, and makes near-perfect decisions when selecting between \McSplit\ and \McSplit-Swap.

\begin{figure}[h!]
    \centering
    \subfigure[][Cumulative plot of instances solved] {
        \centering
        \includegraphics*[width=0.52\textwidth]{14-mcsplit-i-undirected/modified-mcsplit-experiment/plots/plots/mcsplit-random-smart-density-cumulative}
        \label{figure:left-vs-smart-density-cumulative}
    }
    \subfigure[][Run times (ms)] {
        \centering
        \includegraphics*[width=0.4\textwidth]{14-mcsplit-i-undirected/modified-mcsplit-experiment/plots/plots/left-vs-smart-random2-jittered}
        \label{figure:left-vs-smart-random2}
    }
    \caption{The run times of \McSplit-SD are faster overall than those of \McSplit\ for the
        random varying-density instances, and almost indistinguishable from those of the virtual
        best solver of \McSplit\ and \McSplit-Swap.}
        \label{figure:left-vs-smart-d-mcis}
\end{figure}

\subsection{Random Graphs With Similar Density and Varying $n$}

We now turn to our family of varying-order instances: random instances
generated with $p=0.3$ and varying values of $n$.
\Cref{figure:order-when-swap} shows a strong relationship between the graphs'
densities and the relative run times of \McSplit\ and \McSplit-Swap: 
\McSplit-Swap
is preferable if and only if $G$ has more vertices than $H$.  (Instances on which both
algorithms reached the time limit are not shown; this explains the blank region
at the top right.)

\begin{figure}[h!]
    \centering
    \includegraphics*[width=0.5\textwidth]{14-mcsplit-i-undirected/modified-mcsplit-experiment/plots/plots/order-when-swap}
    \caption{For our random varying-order instances, is preferable to use \McSplit\ when $G$
        has fewer vertices than $H$, and to use \McSplit-Swap when $G$ has more vertices than $H$.
        The plot shows one point per instance.  Instances for which \McSplit-Swap
        is faster are plotted in red; instances where \McSplit\ is faster
        are plotted in blue.  Dark red and dark blue indicate that \McSplit-Swap and \McSplit,
        respectively, result in run times at least twice as fast as the alternative.}
    \label{figure:order-when-swap}
\end{figure}

\McSplit-SO, an algorithm that swaps the graphs if the order of $G$ is greater
than the order of $H$, is shown in \Cref{McSplitSOAlg}.
\Cref{figure:left-vs-smart-o-mcis} shows the run times of \McSplit,
\McSplit-SO, and the VBS of \McSplit\ and \McSplit-Swap for our varying-order
instances.  The \McSplit-SO
algorithm performs about as well as the VBS, and is not substantially slower
than \McSplit\ on any instance.  However, the improvement provided by
\McSplit-SO is less dramatic than that provided by \McSplit-SD; \McSplit-SO is
seldom more than an order of magnitude faster than \McSplit\ on non-trivial
instances.

\begin{algorithm}[h!]
\DontPrintSemicolon
\nl $\FuncSty{McSplit-SO}(G, H)$ \label{McSplitSOFun} \;
\nl \Begin{
    \nl \If{$n_G > n_H$}{
        \nl $\KwSty{return}$~$\FuncSty{McSplit-Swap}(G,H)$ \;
    }
    \nl $\KwSty{return}$~$\FuncSty{McSplit}(G,H)$
}
    \caption{\McSplit-SO: a version of \McSplit\ that uses vertex counts to decide whether to swap the input graphs.} 
\label{McSplitSOAlg}
\end{algorithm}

\begin{figure}[h!]
    \centering
    \subfigure[][Cumulative plot of instances solved] {
        \centering
        \includegraphics*[width=0.52\textwidth]{14-mcsplit-i-undirected/modified-mcsplit-experiment/plots/plots/mcsplit-random-smart-order-cumulative}
        \label{figure:left-vs-smart-order-cumulative}
    }
    \subfigure[][Run times (ms)] {
        \centering
        \includegraphics*[width=0.4\textwidth]{14-mcsplit-i-undirected/modified-mcsplit-experiment/plots/plots/left-vs-smart-random3-jittered}
        \label{figure:left-vs-smart-random3}
    }
    \caption{The run times of \McSplit-SD are faster overall than those of \McSplit\ for the
        random varying-order instances, and almost indistinguishable from those of the virtual
        best solver of \McSplit\ and \McSplit-Swap.}
        \label{figure:left-vs-smart-o-mcis}
\end{figure}

\section{Explaining the Success of \McSplit-SD and \McSplit-SO}\label{sec:explaining-sd-so}

Why are \McSplit-SO and \McSplit-SD effective?  In this section, we give some
speculative reasons related to the bound calculated at each search node that go
some way towards explaining the success of the swapping heuristics.  In each
case, we start with an example, then broaden the discussion to the general
case.

First, consider \McSplit-SO: why it is useful to select the graph with
fewer vertices as the first argument when calling \McSplit?  Let $G$ be a graph
with $n_G$ vertices and $H$ be a graph with $n_H$ vertices, such that $n_G <
n_H$.  If we call $\McSplit(G,H)$, the first call to $\FuncSty{Search()}$ in
\Cref{McSplitAlg} makes $n_H + 1$ recursive calls to $\FuncSty{Search()}$: one
for each vertex $w \in V(H)$, and a final call where vertex $v$ of $G$ is
rejected.  Thus, the root node of the search tree has $n_H + 1$ children. If we
reverse the order of the graphs and call $\McSplit(H,G)$, a similar argument
shows that the root node of the search tree has $n_G + 1$ children.  Thus ---
at least at the root node and plausibly also deeper in the tree --- the search
tree has a smaller branching factor if we call \McSplit\ with the \emph{larger}
graph first.

If we were to follow the standard constraint programming practice of branching
on the smallest domain first \cite{DBLP:journals/ai/HaralickE80}, we would 
therefore call $\McSplit(H,G)$ --- that is, we would place the larger graph first.
But this is exactly the opposite of the \McSplit-SO strategy.
Therefore, we must look elsewhere to explain why calling \McSplit\ with the small
graph first is effective.  It seems likely that the bound calculated on \lineref{CalcBound}
of \Cref{McSplitAlg} is part of the explanation.

To give a concrete example, consider the example graphs $G$ and $H$ from
\Cref{fig:alg1}, which are reproduced for convenience in \Cref{figure:G-and-H-redux}.
\Cref{figure:search-tree-without-swapping} shows the search tree if --- following the
strategy of \McSplit-SO --- the smaller graph is passed first to \McSplit.
The upper bound is shown at each node.
\Cref{figure:search-tree-with-swapping} shows the search tree if the larger graph
is passed first to \McSplit.  These search trees have 35 and 45 nodes respectively;
as we might expect, passing the smaller graph first results in a smaller search tree.

\begin{figure}[htb]
    \centering
    \subfigure[][$G$ and $H$] {
        \centering
            \scalebox{0.8}{
                \tikz {
                    \begin{scope}[yshift=2.8cm]
                        \graph [nodes={draw, circle, minimum width=.55cm, inner sep=1pt}, circular placement, radius=0.95cm,
                                clockwise=5] {
                                    1,2,3,4,5;
                            1--4; 1--5; 2--3; 2--5; 3--5;
                        };
                    \end{scope}
                    \graph [nodes={draw, circle, minimum width=.55cm, inner sep=1pt}, circular placement, radius=0.95cm,
                            clockwise=6, phase=60] {
                                a,b,c,d,e,f;
                        a--b; a--c; a--e; b--d; b--f; c--d; c--e; c--f; d--f; e--f;
                    };
                    \node at (0,-.5) {};  % for positioning
                }
            }
        \label{figure:G-and-H-redux}
    }
    \subfigure[][The search tree of $\McSplit(G,H)$] {
        \centering
        \scalebox{0.85}{
            \IfNDebug{\input{14a-mcsplit-swapping/branching-example/james-cpp-modified/treegh-b-and-b-highlighted.tex}}
        }
        \label{figure:search-tree-without-swapping}
    }
    \subfigure[][The search tree of $\McSplit(H,G)$] {
        \centering
        \scalebox{0.85}{
            \IfNDebug{\input{14a-mcsplit-swapping/branching-example/james-cpp-modified/treehg-b-and-b-highlighted.tex}}
        }
        \label{figure:search-tree-with-swapping}
    }
    \caption{The search trees of \McSplit\ and \McSplit-Swap on example graphs $G$ and $H$.  The number
            at each search node shows the computed upper bound, and the label on each edge shows the decision
            made at that branch.  Each blue box
            highlights the search tree explored after making the decision not to map a vertex in the first
            call to $\FuncSty{Search}()$.}
        \label{figure:search-trees-with-swapping}
\end{figure}

The blue rectangle on each search tree highlights the subproblem after the final branching
decision in the first call to $\FuncSty{Search}()$ --- the decision not to use the first
vertex in the first graph (vertex $1$ in \Cref{figure:search-tree-without-swapping}
and vertex $a$ in \Cref{figure:search-tree-with-swapping}).  The size difference between
these highlighted subtrees --- 1 vertex and 16 vertices --- is more than enough to account
for the overall difference in size between the two search trees.  The subtree in
\Cref{figure:search-tree-without-swapping} is small because after rejecting a vertex of
the smaller graph, we can reduce the bound to 4 which lets us prune the search tree immediately.
In \Cref{figure:search-tree-without-swapping}, the rejected vertex belongs to the larger
side of the label class, and therefore the bound remains at 5.

This argument generalises: for any two graphs of unequal order,
the final child of the search tree's root node will have a smaller bound
if we call \McSplit\ with the smaller graph first than if we call the algorithm
with the larger graph first.  This provides at least some explanation for the success of
\McSplit-SO.

We now turn to \McSplit-SD.  Why might passing the graph with the lowest density
extremeness first to \McSplit\ result in a smaller search tree?  As an example, we
use graph $G$ from the previous example along with $I_5$, the edgeless graph on five vertices.
These graphs have density extremeness $0$ and $0.5$ respectively --- the
minimum and maximum possible values.  Thus, using the heuristic of \McSplit-SD,
we would expect the search tree of $\McSplit(G, I_5)$ to be smaller than the search
tree of $\McSplit(I_5, G)$.  Indeed, this is the case; the search trees,
which are shown in \Cref{figure:search-trees-gi-with-swapping}, have 44
and 97 nodes respectively.

\begin{figure}[htb]
    \centering
    \subfigure[][$G$ and $I_5$] {
        \centering
            \scalebox{0.67}{
                \tikz {
                    \begin{scope}[yshift=2.5cm]
                        \graph [nodes={draw, circle, minimum width=.55cm, inner sep=1pt}, circular placement, radius=0.95cm,
                                clockwise=5] {
                                    1,2,3,4,5;
                            1--4; 1--5; 2--3; 2--5; 3--5;
                        };
                    \end{scope}
                    \graph [nodes={draw, circle, minimum width=.55cm, inner sep=1pt}, circular placement, radius=0.8cm,
                            clockwise=5] {
                                a,b,c,d,e;
                    };
                    %% \node at (0,-.5) {};  % for positioning
                }
            }
        \label{figure:G-and-I5}
    }
    \subfigure[][The search tree of $\McSplit(G,I_5)$] {
        \centering
        \scalebox{0.73}{
            \IfNDebug{\input{14a-mcsplit-swapping/branching-example/james-cpp-modified/treegi-b-and-b.tex}}
        }
        \label{figure:search-tree-gi-without-swapping}
    }
    \subfigure[][The search tree of $\McSplit(I_5,G)$] {
        \centering
        \scalebox{0.73}{
            \IfNDebug{\input{14a-mcsplit-swapping/branching-example/james-cpp-modified/treeig-b-and-b.tex}}
        }
        \label{figure:search-tree-gi-with-swapping}
    }
    \caption{The search trees of \McSplit\ and \McSplit-Swap on example graphs $G$ and $I_5$.}
        \label{figure:search-trees-gi-with-swapping}
\end{figure}

Why might it be preferable to call \McSplit\ with $I_5$ as the second
rather than the first graph?  We argue
that the reason is similar to the reason we gave for placing the smaller graph
first in our previous discussion.  It is clear that when calling \McSplit\ with
$G$ and $I_5$ (in either order), there will be exactly one label class throughout
search, consisting of the vertices in both graphs that have not been explicitly
rejected by the algorithm and that are not adjacent to any vertex that has already
been mapped.  The side of that label class containing vertices of $G$ will tend to
contain fewer vertices than the side containing vertices of $I_5$, since the vertex set of $I_5$ is
an independent set.  
Now consider a red branch in either search tree, in which a vertex is rejected.
If that vertex is on the smaller side of the label class, the bound at the child node
in the search tree will be one less than the bound at the parent node; otherwise,
the bound at the child node will be equal to the bound at the parent node.  It is
therefore preferable, in our example, to call \McSplit\ with $G$ first.  Indeed,
in \Cref{figure:search-trees-gi-with-swapping} we can see that there are ten
nodes in the search tree of $\McSplit(I_5, G)$ that result from the rejection of
a vertex (i.e. have a red-labelled edge above) and have the same bound as their
parent.  There are no such nodes in the search tree of $\McSplit(I_5, G)$.

The average branching factor of the search tree for $\McSplit(I_5, G)$ is lower
than the average branching factor of the search tree for $\McSplit(G, I_5)$.  This
suggests that with \McSplit-SD, as is the case for \McSplit-SO, there is a trade-off
where the average branching factor suggests we should use one ordering of the
graphs, but this is outweighed by the better bounds computed with the opposite
ordering.

\section{\McSplit-2S: Generalising \McSplit-SD and \McSplit-SO}\label{sec:mcsplit2}

Our arguments in favour of \McSplit-SD and \McSplit-SO both point towards the
benefit of choosing a vertex in the smaller side of a label class to branch on
in the $\FuncSty{Search}()$ function of \McSplit.  This suggests a modified
version of \McSplit\ in which branching may be carried out on vertices of
either graph, with the decision of which side to choose made at each search
node based on which side of label class is larger.  We show this
algorithm, \McSplit-2S (with the name signifying ``two sided''), in
\Cref{McSplit-2SAlg}.

\Linerangeref{ChooseBranchingSideMS2}{LastNewLineMS2} contain the section of
code that is changed from \Cref{McSplitAlg}.
(In addition,
the function $\FuncSty{NewFuture}$, which creates the new label classes
after a vertex assignment, has been extracted to avoid duplication.)
\Lineref{ChooseBranchingSideMS2} chooses which side to branch on.
If the side of label class $\langle \setG, \setH\rangle$ containing
vertices from the first graph is no larger than the other side of the label class,
the algorithm maps a vertex from $\setG$ to each vertex
in $\setH$ in turn (\linerangeref{RemoveVMS2}{EndOfBranch1MS2}), just
as in the standard \McSplit\ algorithm.  Otherwise, the algorithm
maps a vertex from $\setH$ to each vertex in $\setG$ in turn
(\linerangeref{RemovewMS2Swap}{LastNewLineMS2}).

\begin{algorithm}[h!]
\DontPrintSemicolon
\nl $\FuncSty{NewFuture}(\AlgVar{future},v,w)$ \;
\nl \Begin{
\nl    $\AlgVar{future'} \gets \emptyset$ \label{NewFutureMS2} \;
\nl    \For {$\langle \setG',\setH'\rangle \in future$ \label{InnerLoopMS2}}{
\nl        $\setG'' \gets \setG' \cap \N_G(v)$ \label{NewPWithEdgeMS2} \;
\nl        $\setH'' \gets \setH' \cap \N_H(w)$ \;
\nl        \If {$\setG'' \neq \emptyset$ \bf{and} $\setH'' \neq \emptyset$\label{IfNonEmptyMS2}}{
\nl            $\AlgVar{future'} \gets \AlgVar{future'} \cup \{\langle \setG'' , \setH'' \rangle\}$ \label{AddToFutureWithEdgeMS2}}
\nl        $\setG'' \gets (\setG' \cap \invN_G(v))$ \label{NewPWithoutEdgeMS2}  \;
\nl        $\setH'' \gets (\setH' \cap \invN_H(w))$ \;
\nl        \If {$\setG'' \neq \emptyset$ \bf{and} $\setH'' \neq \emptyset$\label{IfNonEmpty2MS2}}{
\nl            $\AlgVar{future'} \gets \AlgVar{future'} \cup \{\langle \setG'' , \setH'' \rangle\}$} \label{InnerLoopEndMS2}
       }
\nl $\KwSty{return}$~$\AlgVar{future'}$ \;
}
\medskip
\nl $\FuncSty{Search}(\AlgVar{future},M)$ \;
\nl \Begin{
%\nl \lIf {$\AlgVar{future} = \emptyset$ \bf{and} $|M| > |\AlgVar{incumbent}|$}
\nl \lIf {$|M| > |\AlgVar{incumbent}|$}{$\AlgVar{incumbent} \gets M$} \label{StoreIncumbentMS2}
%\nl \lIf {$\AlgVar{future} = \emptyset$}{return}
\medskip
\nl $\AlgVar{bound} \gets |M|  + \sum_{\langle \setG,\setH \rangle \in \AlgVar{future}} \min(|\setG|,|\setH|)$ \label{CalcBoundMS2} \;
\nl \lIf {$\AlgVar{bound} \leq |\AlgVar{incumbent}|$}{\KwSty{return}} \label{PruneSearchMS2}
\medskip
\nl $\langle \setG,\setH \rangle \gets \FuncSty{SelectLabelClass}(\AlgVar{future})$ \label{SelectClassMS2} \;
\nl \If{$|\setG| \leq |\setH|$ \label{ChooseBranchingSideMS2}} {
\nl   \LeftComment{Branch as in standard \McSplit} \;
\nl   $v \gets \FuncSty{SelectVertex}(\setG)$ \label{SelectVertexMS2} \;
\nl   \For {$w \in \setH$ \label{WLoopMS2}} {
\nl     $\FuncSty{Search}(\FuncSty{NewFuture}(\AlgVar{future}, v, w),M\cup \{(v,w)\})$ \label{ExpandWithVMS2} \;
  }
\nl   $\setG' \gets \setG \setminus \{v\}$ \label{RemoveVMS2} \;
\nl   $\AlgVar{future} \gets \AlgVar{future} \setminus \{\langle \setG,\setH \rangle\}$\;
\nl   \lIf {$\setG' \neq \emptyset$ \label{EndOfBranch1MS2}} {$\AlgVar{future} \gets \AlgVar{future} \cup \{\langle \setG',\setH \rangle \}$}
      } \nl \Else {
\nl   \LeftComment{Swapped version: branch on a vertex of $H$} \;
\nl   $w \gets \FuncSty{SelectVertex}(\setH)$ \label{SelectVertexMS2Swap} \;
\nl   \For {$v \in \setG$ \label{vLoopMS2Swap}} {
\nl     $\FuncSty{Search}(\FuncSty{NewFuture}(\AlgVar{future}, v, w),M\cup \{(v,w)\})$ \label{ExpandWithwMS2Swap} \;
  }
\nl   $\setH' \gets \setH \setminus \{w\}$ \label{RemovewMS2Swap} \;
\nl   $\AlgVar{future} \gets \AlgVar{future} \setminus \{\langle \setG,\setH \rangle\}$\;
\nl   \lIf {$\setH' \neq \emptyset$ \label{LastNewLineMS2}} {$\AlgVar{future} \gets \AlgVar{future} \cup \{\langle \setG,\setH' \rangle \}$}
    }
\nl $\FuncSty{Search}(\AlgVar{future},M)$ \label{ExpandWithoutVMS2} \;
}
\;
\nl $\FuncSty{McSplit-2S}(\graphG,\graphH)$ \label{McSplitFunMS2} \;
\nl \Begin{
    \nl $\KwSty{global}~\AlgVar{incumbent} \gets \emptyset$ \;
\nl $\FuncSty{Search}(\{\langle V(\graphG),V(\graphH) \rangle \},\emptyset)$ \label{FirstExpandCallMS2} \;
\nl $\KwSty{return}$~$\AlgVar{incumbent}$ \;
}
\caption{McSplit-2S: a branch-and-bound algorithm to find a maximum common induced subgraph of two graphs.}
\label{McSplit-2SAlg}
\end{algorithm}

In addition to \McSplit-2S, we implemented the reverse heuristic, which replaces the condition
$|G| \leq |H|$ on \Cref{ChooseBranchingSideMS2} with $|G| > |H|$.  This algorithm,
which we would expect to perform poorly, is called \McSplit-2S'.

\Cref{tab:mcsplit-variants} summarises the variants of \McSplit\ that we compare in this section.

\FloatBarrier

\begin{table}[h!]
\centering
 \footnotesize
 \begin{tabular}{p{0.18\linewidth} p{0.45\linewidth} p{0.14\linewidth}}
 \toprule
    Name & Description & Pseudocode \\ [0.5ex]
 \midrule
    \McSplit & The base \McSplit\ algorithm & \Cref{McSplitAlg} \\
    \rule{0pt}{2.3ex}\McSplit-SD & \McSplit\ with $G$ and $H$ swapped if and only if
    	        $\left|\frac{1}{2} - d_G\right| > \left|\frac{1}{2} - d_H\right|$ & \Cref{McSplitSDAlg} \\
    \rule{0pt}{2.3ex}\McSplit-SO & \McSplit\ with $G$ and $H$ swapped if and only if $n_G > n_H$ & \Cref{McSplitSOAlg} \\
    \rule{0pt}{2.3ex}\McSplit-2S & \McSplit\ with branching on both sides & \Cref{McSplit-2SAlg} \\
    \rule{0pt}{2.3ex}\McSplit-2S' & The same as \McSplit-2S, but with the condition for branching on a vertex of $H$ rather than
    		$G$ (\lineref{ChooseBranchingSideMS2}) negated & - \\
 \bottomrule
\end{tabular}
\caption{Summary of \McSplit\ variants used in this chapter's experiments}
\label{tab:mcsplit-variants}
\end{table}

\FloatBarrier

\Cref{figure:mcsplit2-cumulative-random2} shows a cumulative plot comparing \McSplit, \McSplit-2S, \McSplit-2S',
and \McSplit-SD on our family of 24-vertex graphs with varying density.  \McSplit-SD is fastest of
the four algorithms, followed by \McSplit-2S.

\Cref{figure:mcsplit2-cumulative-random3} compares \McSplit, \McSplit-2S, \McSplit-2S',
and \McSplit-SO on our family of graphs with similar density but varying vertex count.
\McSplit-SO and \McSplit-2S have almost identical results.

\Cref{figure:mcsplit2-cumulative-mixandmatch} shows results for a new family of instances,
the ``mix and match'' MCS Plain instances.  Each of these instances is generated by selecting two
MCS Plain instances at random, and using graph $G$ from the first instance and graph $H$ from the second
instance.  These compensate for what may be viewed as a weakness of the MCS Plain instances: that
the two graphs in each instance are produced using the same generator and the same parameter values.
On these ``mix and match'' instances, \McSplit-SD, \McSplit-SO, and \McSplit-2S have similar performance, and
all are clearly faster overall than \McSplit.  A partial explanation for the similar performance
of \McSplit-SD and \McSplit-SO on these instances is that density and degree are negatively correlated
for this set of instances.  As a result, the two swapping rules agree --- and thus the two
algorithms behave identically --- on 745 of the 1000 instances.  TODO CHECK THIS NUMBER 745

\Cref{figure:mcsplit2-cumulative-random4} introduces another new set of random
instances, with both density and order varying.  Each graph is a $G(n,p)$
random graph, with $n$ selected at random from the set $\{10, 11, \dots, 40\}$
and $p$ chosen uniformly at random from the interval $[0,1]$.  Of the five
algorithms, \McSplit-SD is the clear winner.  Unlike in the three previous
plots, \McSplit-2S does not clearly perform better overall than \McSplit;
indeed, these two algorithms and \McSplit-SO have very similar cumulative
curves.  The unremarkable performance of \McSplit-2S on these instances is
something of a puzzle.\footnote{My conjecture is that \McSplit-2S \emph{would}
outperform \McSplit\ if the initial sorting by degree of graphs $G$ and $H$
took into account the densities of both graphs in a way that made sense for the
two-sided algorithm. In a small preliminary experiment in which the sorting of
graph $G$ depended on the density of graph $H$ and vice versa, \McSplit-2S
outperformed \McSplit, although \McSplit-SD remained the fastest solver.
I leave further investigation of strategies for sorting by degree in \McSplit-2S
as future work.} 

%Overall, these results may be viewed as a partial success story for \McSplit-2S.  For each of the four families
%of graphs, \McSplit-2S is faster overall than \McSplit, and its performance is not far from that of the better
%of \McSplit-SD and \McSplit-SO.  However, it may be viewed as slightly disappointing that \McSplit-2S is not
%the overall winner for any of these four graph classes, despite being able to branch on either side at each
%search node.

\begin{figure}[htb]
    \centering
    \subfigure[][Random instances, varying density] {
        \centering
        \includegraphics*[width=0.46\textwidth]{14-mcsplit-i-undirected/modified-mcsplit-experiment/plots/plots/random2-mcsplit2-cumulative}
        \label{figure:mcsplit2-cumulative-random2}
    }
    \subfigure[][Random instances, varying order] {
        \centering
        \includegraphics*[width=0.46\textwidth]{14-mcsplit-i-undirected/modified-mcsplit-experiment/plots/plots/random3-mcsplit2-cumulative}
        \label{figure:mcsplit2-cumulative-random3}
    }
    \subfigure[][``Mix and match'' MCS Plain instances] {
        \centering
        \includegraphics*[width=0.46\textwidth]{14-mcsplit-i-undirected/modified-mcsplit-experiment/plots/plots/mixandmatch-cumulative}
        \label{figure:mcsplit2-cumulative-mixandmatch}
    }
    \subfigure[][Random instances, varying density and order] {
        \centering
        \includegraphics*[width=0.46\textwidth]{14-mcsplit-i-undirected/modified-mcsplit-experiment/plots/plots/random4-mcsplit2-cumulative}
        \label{figure:mcsplit2-cumulative-random4}
    }
    %\subfigure[][Barabasi-Albert graphs] {
    %    \centering
    %    \includegraphics*[width=0.46\textwidth]{14-mcsplit-i-undirected/modified-mcsplit-experiment/plots/plots/ba-cumulative}
    %    \label{figure:mcsplit2-cumulative-ba}
    %}
    %\subfigure[][$G(n,m)$ graphs] {
    %    \centering
    %    \includegraphics*[width=0.46\textwidth]{14-mcsplit-i-undirected/modified-mcsplit-experiment/plots/plots/gnm-cumulative}
    %    \label{figure:mcsplit2-cumulative-gnm}
    %}
    \caption{Cumulative plots comparing \McSplit, \McSplit-2S, and the swapping versions of \McSplit\ on four graph classes.}
    \label{figure:mcsplit2-cumulative}
\end{figure}

On all four instance families shown in \Cref{figure:mcsplit2-cumulative}, \McSplit-2S'
is the worst-performing algorithm.  This is surprising in one sense, as \McSplit-2S' is the algorithm
of the five used that comes closest to a smallest-domain-first heuristic.  However, the poor performance
of \McSplit-2S' is explained by our foregoing discussion of the effect of branching side on upper
bounds.

\subsection{Pairs of Graphs With Equal Density and Order}

In the previous section, we saw that \McSplit-2S is substantially faster overall than \McSplit\ for families
of instances in which the pattern and target graph differ in density or order.  Yet the results in that
section could be viewed as somewhat disappointing; \McSplit-2S is outperformed by \McSplit-SD on instances
whose graphs have different densities, and by \McSplit-SO on instances whose graphs have similar density
but different numbers of vertices.  Are there families of instances for which \McSplit-2S clearly outperforms
both \McSplit-SD and \McSplit-SO?  To examine this question, this section considers three families of instances
in which graphs $G$ and $H$ have the same number of vertices and equal density.  For these families
of instances, \McSplit-SD and \McSplit-SO behave identically to
\McSplit.  Thus, if \McSplit-2S outperforms \McSplit\ on any of these families, it also
outperforms both \McSplit-SD and \McSplit-SO on that family.

The first of our equal-density, equal-order families of instances contains pairs of Erd\H{o}s-Rényi $G(n,m)$ graphs.  For each instance,
$n$ is chosen uniformly at random from $\{15, \dots, 40\}$, then $m$ is chosen
uniformly at random from the range of possible edge counts: $\{0, \dots, n(n-1)/2\}$.  Each of the graphs
$G$ and $H$ is generated by creating a set of $n$ vertices, then adding
a random $m$-element subset of the possible edges
between pairs of vertices.

\Cref{figure:mcsplit-vs-mcsplit2-gnm} shows a cumulative plot and a scatter
plot of run times of \McSplit\ and \McSplit-2S.  The cumulative plot also shows
results for \McSplit-2S'.  The results are disappointing. All three
algorithms have very similar performance overall;
worse still, \McSplit-2S exceeds the time limit on some instances that are
easy for \McSplit.\footnote{All of these outliers are very sparse or very dense.
The reason for the difference between the algorithms' run times on these instances
remains unclear.}
A small point in favour of \McSplit-2S is that the algorithm is
slightly faster than \McSplit\ for many of the hard instances,
but the differences in run times are almost imperceptible on the scatter plot.  

\begin{figure}[htb]
    \centering
    \subfigure[][Cumulative plot] {
	\centering
        \includegraphics*[width=0.52\textwidth]{14-mcsplit-i-undirected/modified-mcsplit-experiment/plots/plots/gnm-cumulative}
        \label{figure:mcsplit-vs-mcsplit2-gnm-cumulative}
    }
    \subfigure[][Scatter plot with one point per instance] {
	\centering
	\includegraphics*[width=0.40\textwidth]{14-mcsplit-i-undirected/modified-mcsplit-experiment/plots/plots/mcsplit-vs-mcsplit2-gnm-jittered}
        \label{figure:mcsplit-vs-mcsplit2-gnm-scatter}
    }
    \caption{Comparison of \McSplit\ and \McSplit-2S on $G(n,m)$ graphs}
    \label{figure:mcsplit-vs-mcsplit2-gnm}
\end{figure}

Our second instance generator creates a pair of graphs using the Barabási-Albert model \citep{barabasi1999emergence},
which is very widely cited in the field of network science.
We chose this model because, as with the $G(n,m)$ model but unlike many other random graph models such
as $G(n,p)$, we can fix the exact density of the generated graphs.
The Barabási-Albert model generates graphs with a wide distribution of degrees, allowing us to test
our algorithms on random graphs that are quite different in structure from those generated by
an Erd\H{o}s-Rényi model.
The Barabási-Albert generator has two parameters, $n$ and $m$, such that
$1 \leq m < n$.  The generator works as follows: begin with the star $K_{1,m}$.
Extra vertices are added one by one until the graph has $n$ vertices.  Each time a vertex $v$ is added,
$m$ edges are added from $v$ to earlier vertices.  These edges are added at random, but using a ``preferential
attachment'' mechanism: at each step, vertices with more existing edges are proportionately more likely to have a new edge
added.  We use the implementation of this generator from the Python package NetworkX \citep{networkx}.
For each instance, we use the following ranges of parameter values in order to
have wide variation in order and density between instances: $n$ is chosen at random
from $\{10,\dots,50\}$, and $m$ is chosen at random from $\{2,\dots,n-1\}$.  Within each instance, the two
generated graphs use the same parameter values $n$ and $m$.
\Cref{figure:mcsplit-vs-mcsplit2-ba} compares \McSplit\ with \McSplit-2S on these Barabási-Albert instances.
Overall, \McSplit-2S has a small speed advantage over \McSplit.  The scatter plot shows that the two algorithms
tend to have very similar run times, with \McSplit-2S the slightly faster algorithm in most cases.

\begin{figure}[htb]
    \centering
    \subfigure[][Cumulative plot] {
	\centering
        \includegraphics*[width=0.52\textwidth]{14-mcsplit-i-undirected/modified-mcsplit-experiment/plots/plots/ba-cumulative}
        \label{figure:mcsplit-vs-mcsplit2-ba-cumulative}
    }
    \subfigure[][Scatter plot with one point per instance] {
	\centering
	\includegraphics*[width=0.40\textwidth]{14-mcsplit-i-undirected/modified-mcsplit-experiment/plots/plots/mcsplit-vs-mcsplit2-ba-jittered}
        \label{figure:mcsplit-vs-mcsplit2-ba-scatter}
    }
    \caption{Comparison of \McSplit\ and \McSplit-2S on Barabási-Albert graphs}
    \label{figure:mcsplit-vs-mcsplit2-ba}
\end{figure}

The goal of our final instance generator in this section is to produce two graphs with very different
characteristics, but with equal orders and densities.  Each instance in this family
consists of one Barabási-Albert graph and one $G(n,m)$ graph, with the parameters for the latter
generator chosen to match the order and density of the Barabási-Albert graph.  A ``coin flip''
is used to determine whether $G$ is the Barabási-Albert graph or the $G(n,m)$ graph.
\Cref{figure:mcsplit-vs-mcsplit2-ba-gnm}
shows the results of running our algorithms on these instances.  The cumulative curve shows that \McSplit-2S is, by a small margin, the
fastest algorithm overall, and that \McSplit-2S' is the slowest.  \McSplit-2S and \McSplit\ exceeded
the time limit on 105 and 127 instances respectively.  The scatter plot shows that the two
algorithms often have very different run times, with \McSplit\ outperforming \McSplit-2S on several instances.

\begin{figure}[htb]
    \centering
    \subfigure[][Cumulative plot] {
	\centering
        \includegraphics*[width=0.52\textwidth]{14-mcsplit-i-undirected/modified-mcsplit-experiment/plots/plots/ba-gnm-cumulative}
        \label{figure:mcsplit-vs-mcsplit2-ba-gnm-cumulative}
    }
    \subfigure[][Scatter plot with one point per instance] {
	\centering
	\includegraphics*[width=0.40\textwidth]{14-mcsplit-i-undirected/modified-mcsplit-experiment/plots/plots/mcsplit-vs-mcsplit2-ba-gnm-jittered}
        \label{figure:mcsplit-vs-mcsplit2-ba-gnm-scatter}
    }
    \caption{Comparison of \McSplit\ and \McSplit-2S on $G(n,m)$ / Barabási-Albert graphs pairs}
    \label{figure:mcsplit-vs-mcsplit2-ba-gnm}
\end{figure}

This section has shown a modest win for \McSplit-2S; for our two families of
instances using the Barabási-Albert model, \McSplit-2S slightly outperforms \McSplit\ --- and therefore
also outperforms \McSplit-SD and \McSplit-SO.

%\begin{figure}[h!]
%    \centering
%    \includegraphics*[width=0.5\textwidth]{14-mcsplit-i-undirected/modified-mcsplit-experiment/plots/plots/mcsplit-vs-mcsplit2-gnm}
%    \caption{TODO}
%        \label{figure:mcsplit-vs-mcsplit2-gnm}
%\end{figure}

\section{Related Work}\label{sec:swapping-related-work}

I have not been able to find any prior work on maximum common subgraph problems in which
graphs are swapped using any criterion other than number of vertices.  Nor have I found
any systematic evaluation of when graphs should be swapped.

A number of papers on maximum common subgraph problems make the assumption, consistent
with \McSplit-SO, that $n_G \leq n_H$.  An early example
is McGregor's paper on the maximum common edge subgraph problem \cite{DBLP:journals/spe/McGregor82},
where no rationale for the ordering of graphs is given.
Hoffmann, McCreesh and Reilly \cite{UpcomingAAAIPaper} put the smaller graph first
in the \kDown algorithm with no explanation; the presumable reason is that \kDown.
is based on a subgraph isomorphism algorithm which requires the smaller graph to be first.
Dalke and Hastings \cite{DBLP:journals/jcheminf/DalkeH13} place the smallest
graph first in FMCS, an algorithm for the maximum
common \emph{edge} subgraph problem.\footnote{More precisely, the algorithm chooses a graph whose largest
connected component is as small as possible, as FMCS searches only for connected subgraphs.}
A short explanation is given in a code comment \cite{DalkeFmcsSource}. To summarise: this strategy tends to
result in fewer subgraphs of the first graph being visited.  This explanation is very
important in FMCS, which makes a call to a subgraph isomorphism algorithm for every visited
subgraph, but is not directly applicable to \McSplit.

I have not found any prior maximum common subgraph algorithm that branches on both graphs,
as \McSplit-2S does.  However, it is common in constraint programming to combine multiple models
for a problem, with ``channeling'' constraints to ensure consistency between these models
\cite{DBLP:journals/constraints/ChengCLW99}.  In effect, the data structures of \McSplit\
give us access to a second model --- the ``dual'' model with the roles of variables and values
reversed \cite{DBLP:conf/ecai/Geelen92} --- without any need to explicitly store additional
domains or constraints, and therefore with no overhead in memory use
or run time.

\section{Conclusion}\label{sec:swapping-conclusion}

In this chapter, we have introduced two rules to determine whether to swap graphs $G$ and $H$
when calling \McSplit: one using the graphs' densities, and the other using their node counts.
We have shown that both rules are very effective in reducing the run time of \McSplit\ both on random
graphs and on ``mix and match'' instances created by choosing random pairs of graphs from the
MCS Plain instances.  Using insights from the behaviour of these versions of \McSplit, we have
introduced \McSplit-2S, a version of \McSplit\ that branches on vertices of both graphs.
This achieves similar performance to \McSplit-SD and \McSplit-SO on several graph
families, and there exist families of graphs on which it outperforms both
algorithms.

There are many potential avenues for future work.  We could investigate
whether swapping rules are useful for other maximum common subgraph algorithms, including
algorithm for maximum common \emph{edge} subgraph which are beyond the scope of this
dissertation.
We could also explore rules for swapping graphs that take into account both density and order,
perhaps along with additional characteristics of the two graphs.

With \McSplit-2S, we have only begun to explore the possibilities of branching on both graphs.
We could investigate replacing our simple ``branch on the larger side'' heuristic with more
sophisticated rules for choosing which side to branch on at each search node.  We could also
combine the two-sided branching of \McSplit-2S with the initial swapping rules of \McSplit-SD
or \McSplit-SO.

Towards the end of next chapter, we will return to the MCIS problem with
specialised algorithm for large, sparse graphs.  All of the instances in that
chapter have $|V(G)| < |V(H)|$, so we will implicitly use the \McSplit-SO
swapping heuristic. First, we turn to our second problem: induced subgraph
isomorphism.
